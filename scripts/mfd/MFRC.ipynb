{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4acd055",
   "metadata": {},
   "source": [
    "# Moral foudations Redidt corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1388d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import ast\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c06f455",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba885b63",
   "metadata": {},
   "source": [
    "Ensure that the `final_mfrc_data.csv` file is in the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e527f7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>bucket</th>\n",
       "      <th>annotator</th>\n",
       "      <th>annotation</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That particular part of the debate is especial...</td>\n",
       "      <td>europe</td>\n",
       "      <td>French politics</td>\n",
       "      <td>annotator03</td>\n",
       "      <td>Non-Moral</td>\n",
       "      <td>Confident</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text subreddit  \\\n",
       "0  That particular part of the debate is especial...    europe   \n",
       "\n",
       "            bucket    annotator annotation confidence  \n",
       "0  French politics  annotator03  Non-Moral  Confident  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfrc = pd.read_csv(\"data/final_mfrc_data.csv\")\n",
    "mfrc.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c963f97",
   "metadata": {},
   "source": [
    "- Make sentences unique\n",
    "- Map moral domains to MFD (needs double-checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de808d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 61226/61226 [00:24<00:00, 2516.14it/s]\n"
     ]
    }
   ],
   "source": [
    "if True:#  not os.path.isfile(\"data/mfrc_preprocessed.csv\"):\n",
    "    mfrc_unique = pd.DataFrame()\n",
    "    mfrc_unique[\"text\"] = mfrc.text.unique()\n",
    "    \n",
    "    for i in [\"authority\", \"care\", \"fairness\", \"loyalty\", \"sanctity\", \"none\"]:\n",
    "        mfrc_unique[i] = 0\n",
    "    mfrc_unique.set_index(\"text\", inplace=True)\n",
    "\n",
    "    key_map = {\n",
    "        \"Thin Morality\": \"none\",\n",
    "        \"Non-Moral\": \"none\",\n",
    "        \"Care\": \"care\",\n",
    "        \"Purity\": \"sanctity\",\n",
    "        \"Authority\": \"authority\",\n",
    "        \"Loyalty\": \"loyalty\",\n",
    "        \"Proportionality\": \"fairness\",\n",
    "        \"Equality\": \"fairness\"\n",
    "    }\n",
    "    for i, row in tqdm(mfrc.iterrows(), dynamic_ncols=True,\n",
    "                       total=len(mfrc)):\n",
    "        text, fs = row[\"text\"], row[\"annotation\"].split(\",\")\n",
    "        for f in fs:\n",
    "            mfrc_unique.loc[text, key_map[f]] += 1\n",
    "            \n",
    "\n",
    "    # Save\n",
    "    mfrc_unique[\"text\"] = mfrc_unique.index\n",
    "    mfrc_unique.index = range(len(mfrc_unique))\n",
    "    mfrc_unique[\"tokens\"] = mfrc_unique[\"text\"].map(lambda y: [x.text.lower().strip() for x in nlp(y)\n",
    "                                                             if x.text.lower().strip().isalpha() and\n",
    "                                                             len(x.text.lower().strip()) >= 3])\n",
    "    mfrc_unique.to_csv(\"data/mfrc_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec9abe1",
   "metadata": {},
   "source": [
    "## Score using MFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f54b82b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrc = pd.read_csv(\"data/mfrc_preprocessed.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbd7f2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicons = [\"mfd\", \"mfd2\", \"emfd\"]\n",
    "scores = {}\n",
    "for lexicon in lexicons:\n",
    "    scores[lexicon] = pd.read_csv(f\"data/mfrc_scoring_results/mfrc_{lexicon}.csv\", index_col=0)\n",
    "foundations = [\"authority\", \"care\", \"fairness\", \"loyalty\", \"sanctity\"]\n",
    "for lexicon in lexicons:\n",
    "    for foundation in foundations:\n",
    "        y = (mfrc[foundation] > 0).astype(int)\n",
    "        scores_df = scores[lexicon]\n",
    "        y_score = scores_df[foundation]\n",
    "        y_pred = np.array(y_score >= y_score.median(), dtype=int)\n",
    "        \n",
    "        eval = {\n",
    "            \"y_true\": y.tolist(),\n",
    "            \"y_pred\": y_pred.tolist(),\n",
    "            \"y_score\": y_score.tolist()\n",
    "        }\n",
    "\n",
    "        with open(f\"data/mfrc_scoring_results/by_foundation/{lexicon}_{foundation}.json\", \"w\") as f:\n",
    "            json.dump(eval, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b8b21",
   "metadata": {},
   "source": [
    "## Predict using logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72244b9e",
   "metadata": {},
   "source": [
    "Embed the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccbfc743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer,\n",
    "    TfidfTransformer\n",
    ")\n",
    "from scipy.sparse import save_npz, load_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "953d95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf\n",
    "if not os.path.isfile(\"data/MFRC/embeddings/tfidf.npz\"):\n",
    "    sentences = mfrc.text.unique()\n",
    "    tokens = mfrc_unique.tokens\n",
    "\n",
    "    # tfidf\n",
    "    with open(\"emfd/data/embeddings/tfidf_vocab.pkl\", \"rb\") as f:\n",
    "        tfidf_vocab = pickle.load(f)\n",
    "    tfidf_vec = TfidfVectorizer(tokenizer=lambda tokens: tokens, \n",
    "                                lowercase=False, stop_words=None,\n",
    "                                min_df=3, \n",
    "                                max_df=0.99,\n",
    "                                vocabulary=tfidf_vocab\n",
    "                                )\n",
    "    X_tfidf = tfidf_vec.fit_transform(tokens)\n",
    "    save_npz(\"data/MFRC/embeddings/tfidf.npz\", X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ac00b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy\n",
    "if not os.path.isfile(\"data/MFRC/embeddings/spacy_300.npz\"):\n",
    "    X_spacy = np.zeros((len(sentences), 300), dtype=float)\n",
    "    for i, doc in tqdm(enumerate(nlp.pipe(sentences, n_process=-1))):\n",
    "        X_spacy[i, :] = doc.vector\n",
    "    np.savez(\"data/MFRC/embeddings/spacy_300.npz\", X_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9a059b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove\n",
    "if not os.path.isfile(\"data/MFRC/embeddings/glove_twitter_200.npz\"):\n",
    "    from utils import make_one_concept\n",
    "    from gensim.models import KeyedVectors\n",
    "    # load the Stanford GloVe model\n",
    "    glove_filename = \"data/embeddings/glove.twitter.27B.200d\"\n",
    "    word2vec_output_file = glove_filename+'.word2vec'\n",
    "    glove = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "    X_glove = np.zeros((len(sentences), 200))\n",
    "    for i, doc in tqdm(enumerate(nlp.pipe(sentences, n_process=-1)), \n",
    "                       desc=\"Processed\", disable=False, \n",
    "                       dynamic_ncols=True, unit=\" docs\"):\n",
    "        tokens = [tok.lemma_.lower() for tok in doc]\n",
    "        X_glove[i] = make_one_concept(model=glove, word_list=tokens,\n",
    "                                    normalize=True)\n",
    "    np.savez(\"data/MFRC/embeddings/glove_twitter_200.npz\", X_glove)\n",
    "    del glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f807b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"data/MFRC/embeddings/sentence_roberta.npz\"):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(\"stsb-roberta-large\")\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "    X_bert = model.encode(sentences)\n",
    "    np.savez(\"data/MFRC/embeddings/sentence_roberta.npz\", X_bert)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c03cca",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdb427d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer,\n",
    "    TfidfTransformer\n",
    ")\n",
    "\n",
    "with open(\"emfd/data/embeddings/tfidf_vocab.pkl\", \"rb\") as f:\n",
    "    tfidf_vocab = pickle.load(f)\n",
    "\n",
    "embs = {\n",
    "    \"tfidf\": load_npz(\"data/MFRC/embeddings/tfidf.npz\"),\n",
    "    \"spacy\": np.load(\"data/MFRC/embeddings/spacy_300.npz\")[\"arr_0\"],\n",
    "    \"bert\": np.load(\"data/MFRC/embeddings/sentence_roberta.npz\")[\"arr_0\"],\n",
    "    \"glove\": np.load(\"data/MFRC/embeddings/glove_twitter_200.npz\")[\"arr_0\"]\n",
    "}\n",
    "def get_model_and_data(emb_name, foundation):\n",
    "    # Load model\n",
    "    model_name = f\"logreg_{emb_name}_{foundation}\"\n",
    "    model_path = f\"emfd/data/sentence_classifiers/{model_name}.pkl\"\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    model.set_params(**{\"n_jobs\": -1})\n",
    "    \n",
    "    # Load data\n",
    "    X = embs[emb_name]\n",
    "    if foundation == \"none\":\n",
    "        y = np.array(mfrc_unique[[\"care\", \"authority\", \"fairness\", \"loyalty\", \"sanctity\"]].max(1) == 0, dtype=int)\n",
    "    else:\n",
    "        y = (mfrc_unique[foundation] > 0).astype(int)\n",
    "    \n",
    "    return model, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf36cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "foundations = [\"authority\", \"care\", \"fairness\", \"loyalty\", \"sanctity\", \"none\"]\n",
    "emb_names = [\"tfidf\", \"spacy\", \"bert\", \"glove\"]\n",
    "for emb_name in emb_names:\n",
    "    for foundation in foundations:\n",
    "        model, X, y = get_model_and_data(emb_name=emb_name, foundation=foundation)\n",
    "        y_true = y.tolist()\n",
    "        y_score = model.predict_proba(X)[:, 1].tolist()\n",
    "        y_pred = model.predict(X).tolist()\n",
    "\n",
    "        evals = {\n",
    "            \"y_true\": y_true,\n",
    "            \"y_pred\": y_pred,\n",
    "            \"y_score\": y_score\n",
    "        }\n",
    "        \n",
    "        model_name = f\"logreg_{emb_name}_{foundation}\"\n",
    "        \n",
    "        with open(f\"data/MFRC/eval_results/{model_name}.json\", \"w\") as f:\n",
    "            json.dump(evals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa713ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a83c7f9c0b00199977994ef8b792d191275126a21078cfa6a3899708a2d04dc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

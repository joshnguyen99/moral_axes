{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baefaace",
   "metadata": {},
   "source": [
    "# Utils for attention annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb2fb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a28d0",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "Change tokenizer and encoder to the desired models, e.g., by loading a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "503c64b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "encoder = AutoModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d1206",
   "metadata": {},
   "source": [
    "## Feed a text through model\n",
    "\n",
    "- Tokenize the text\n",
    "- Feed through encoder\n",
    "- Get the attention score for the first token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c1a09b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<s>          0.323200\n",
       "This         0.015502\n",
       "Ġis          0.019581\n",
       "Ġan          0.017148\n",
       "Ġexample     0.018047\n",
       "Ġsentence    0.034983\n",
       ",            0.025972\n",
       "Ġwhich       0.026652\n",
       "Ġyou         0.030326\n",
       "Ġwill        0.025305\n",
       "Ġreplace     0.023227\n",
       "Ġlater       0.021675\n",
       ".            0.310481\n",
       "</s>         0.107901\n",
       "dtype: float32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"This is an example sentence, which you will replace later.\"\"\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "# Encode\n",
    "outputs = encoder(**inputs, output_attentions=True)\n",
    "\n",
    "# Encoding of text = encoding of the first token\n",
    "encoding = outputs[0][:, 0, :]\n",
    "\n",
    "# Get the attention scores for the first token\n",
    "attns = outputs.attentions[-1].mean(dim=1)[0][0].cpu().detach().numpy()\n",
    "\n",
    "pd.Series(attns, index=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d47ce9",
   "metadata": {},
   "source": [
    "## Reconstruct from list of tokens\n",
    "\n",
    "Since we may have sub-word tokens, we will reconstruct the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d4c9e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input = token\n",
    "# Outputs:\n",
    "#  - the token stripped of the special character\n",
    "#  - whether the token is the start of a new word\n",
    "#\n",
    "# This is an example for RoBERTa, where if a token starts\n",
    "# with \"Ġ\", then it is the beginning of a new word.\n",
    "#\n",
    "# TODO: Change this for BERT or another model.\n",
    "def token_strip(token):\n",
    "    cont_char = \"Ġ\"\n",
    "    starts_with_cont = token.startswith(cont_char)\n",
    "    stripped_token = token\n",
    "    if starts_with_cont:\n",
    "        stripped_token = token[len(cont_char):]\n",
    "    is_new_token = starts_with_cont\n",
    "    return stripped_token, is_new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63548f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip_left_special: if true, remove the <s> token for RoBERTa\n",
    "# strip_right_special: if true, remove the </s> token for RoBERTa\n",
    "#\n",
    "# Works the same for other models, where the BOS and EOS tokens \n",
    "# are different.\n",
    "def tokens_to_string_with_attn(tokens, scores,\n",
    "                               strip_left_special=True,\n",
    "                               strip_right_special=True,\n",
    "                               strip_fn=token_strip):\n",
    "    if strip_left_special:\n",
    "        tokens = tokens[1:]\n",
    "        scores = scores[1:]\n",
    "    if strip_right_special:\n",
    "        tokens = tokens[:-1]\n",
    "        scores = scores[:-1]\n",
    "    token_list = []\n",
    "    score_list = []\n",
    "    curr_token = None\n",
    "    curr_scores = []\n",
    "    for token, score in zip(tokens, scores):\n",
    "        token, is_new_token = strip_fn(token)\n",
    "        if not is_new_token:\n",
    "            if curr_token is None:\n",
    "                curr_token = \"\"\n",
    "            curr_token += token\n",
    "            curr_scores.append(score)\n",
    "        else:\n",
    "            if curr_token is not None:\n",
    "                token_list.append(curr_token)\n",
    "                # Take the average of the attention scores\n",
    "                # for all sub-word tokens of the word\n",
    "                score_list.append(np.mean(curr_scores))\n",
    "            curr_token = token\n",
    "            curr_scores = [score]\n",
    "    if len(curr_scores) > 0:\n",
    "        token_list.append(curr_token)\n",
    "        score_list.append(np.mean(curr_scores))\n",
    "    return token_list, score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee9ff53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This         0.015502\n",
       "is           0.019581\n",
       "an           0.017148\n",
       "example      0.018047\n",
       "sentence,    0.030478\n",
       "which        0.026652\n",
       "you          0.030326\n",
       "will         0.025305\n",
       "replace      0.023227\n",
       "later.       0.166078\n",
       "dtype: float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list, score_list = tokens_to_string_with_attn(tokens=tokens,\n",
    "                                                    scores=attns,\n",
    "                                                    strip_left_special=True,\n",
    "                                                    strip_right_special=True)\n",
    "pd.Series(score_list, index=token_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
